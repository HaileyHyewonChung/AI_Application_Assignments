{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Network\n",
        "\n",
        "### Imports"
      ],
      "metadata": {
        "id": "qES8gFbcssfB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk3zFIPMlbYc",
        "outputId": "e272c151-669a-4459-cc6a-9be2d6e9a30b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb5ab3ff310>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Construction"
      ],
      "metadata": {
        "id": "b6WxAy5OszDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary\n",
        "sample_sentence = 'hi!hello.'\n",
        "char_set = list(set(sample_sentence))\n",
        "dic = {c: i for i, c in enumerate(char_set)}\n",
        "\n",
        "# Parameters\n",
        "dic_size = len(dic)\n",
        "input_size = dic_size\n",
        "hidden_size = dic_size\n",
        "\n",
        "# Dataset setting\n",
        "x_batch = []\n",
        "y_batch = []\n",
        "\n",
        "x_data = [dic[c] for c in sample_sentence[:-1]]\n",
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
        "y_data = [dic[c] for c in sample_sentence[1:]]\n",
        "\n",
        "x_batch.append(x_one_hot)\n",
        "y_batch.append(y_data)\n",
        "\n",
        "# To torch tensors\n",
        "X = torch.FloatTensor(x_batch)\n",
        "Y = torch.LongTensor(y_batch)\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAqmf9qEsyXi",
        "outputId": "acef81e7-5945-47b8-8053-36220a6f7313"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8, 7])\n",
            "torch.Size([1, 8])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. RNN model\n",
        "\n",
        "- Input(입력의 형태)\n",
        "  + Input type: torch.Tensor\n",
        "  + Input shape: (N x S x E)\n",
        "    * N: Batch size, S: Sequence length, E: Embedding size\n",
        "    * 단, <b>batch_first=True</b>일 때\n",
        "  + 입력값 'hi!hello'\n",
        "    * (1, 8, 7)\n",
        "\n",
        "- Hidden(출력의 형태)\n",
        "  + Hidden type: torch.Tensor\n",
        "  + 출력값 'i!hello'\n",
        "    * (1, 8, 7)\n",
        "    "
      ],
      "metadata": {
        "id": "Bmm78Fx3s6yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "learning_rate = 0.1\n",
        "training_epochs = 50\n",
        "model = nn.RNN(input_size, hidden_size, batch_first=True)"
      ],
      "metadata": {
        "id": "ZD2pU6HOs6Lq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define cost/loss & optimizer\n",
        "criterion = nn.CrossEntropyLoss()    # Softmax\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# train\n",
        "for epoch in range(training_epochs):\n",
        "  optimizer.zero_grad()\n",
        "  outputs, _status = model(X)\n",
        "  loss = criterion(outputs.reshape(-1, dic_size), Y.reshape(-1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 5 == 4:\n",
        "    result = outputs.data.numpy().argmax(axis=2)\n",
        "    result_str = ''.join([char_set[c] for c in np.squeeze(result)])\n",
        "    print('epoch: ',epoch, 'loss: ', loss.item(), 'prediction: ', result_str, 'true Y: ', sample_sentence[1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oOgw3JCtqd5",
        "outputId": "f790250f-3969-48ee-bafd-4f44b95c6637"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  4 loss:  1.3552616834640503 prediction:  i!heel.. true Y:  i!hello.\n",
            "epoch:  9 loss:  0.9755570888519287 prediction:  i!hehlo. true Y:  i!hello.\n",
            "epoch:  14 loss:  0.7983919382095337 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  19 loss:  0.7118602991104126 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  24 loss:  0.6624301671981812 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  29 loss:  0.641640841960907 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  34 loss:  0.6319407820701599 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  39 loss:  0.6269137263298035 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  44 loss:  0.6242977380752563 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  49 loss:  0.6222154498100281 prediction:  i!hello. true Y:  i!hello.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Assignment\n",
        "\n",
        "다음 3개의 문장을 batch data로 활용해 RNN을 학습해보자 (아래의 미완성 코드, 위 실습코드, 실행결과를 참고)\n",
        "\n",
        "- 'howareyou'\n",
        "- 'whats up?'\n",
        "- 'iamgreat'"
      ],
      "metadata": {
        "id": "GrMy9W2qtqAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(100)\n",
        "\n",
        "# Dictionary. 샘플 문장에 쓰인 글자 수가 모두 9로 같다.\n",
        "sample_sentences = ['howareyou', 'whats up?', 'iamgreat.']\n",
        "char_set = list(set(''.join(sample_sentences)))\n",
        "dic = {c: i for i, c in enumerate(char_set)}\n",
        "\n",
        "print(sample_sentences[0])\n",
        "print(len(sample_sentences[0]))\n",
        "print(sample_sentences[1])\n",
        "print(len(sample_sentences[1]))\n",
        "print(sample_sentences[2])\n",
        "print(len(sample_sentences[2]))\n",
        "\n",
        "# 샘플 문장의 수는 세개, 16개의 문자가 char_set에 입력되며, 그 중에는 ' '도 포함된다.\n",
        "print(sample_sentences)\n",
        "print(char_set)\n",
        "print(dic)\n",
        "\n",
        "# Parameters. Input으로 17개의 글자가 들어감으로 17개 ,hidden size = Output Size이기 때문에 글자의 갯수인 17개가 맞다.\n",
        "# 결국 sample_sentences의 문자들이 인풋과 아웃풋의 후보가 되므로, sample_sentences에 쓰인 문자의 갯수인 len(dic)이 결국 인풋과 아웃풋의 사이즈가 되는 것이다.\n",
        "dic_size = len(dic)\n",
        "input_size = dic_size\n",
        "hidden_size = dic_size\n",
        "\n",
        "# 딕셔너리 형으로 정의된 dic의 크기. dic은 char_set을 key로 사용하고, 숫자는 char_set의 순서에 따라 매긴 것이므로 dic과 char_set의 크기는 같다.\n",
        "print(dic_size)\n",
        "print(len(char_set))\n",
        "\n",
        "# Dataset setting. 인풋 batch, 타겟 batch \n",
        "input_batch = []\n",
        "target_batch = []\n",
        "\n",
        "# 각 sentence를 학습시킬 때 for문을 사용하여 순차적으로 howareyou부터 iamgreat까지 순차적으로 학습한다.\n",
        "# 학습시킬 sentence의 차례에는 x_data에 sentence, x_one_hot에 원핫코드, y_data에 정답을 입력한다.\n",
        "# 따라서 모델의 input으로 들어갈 수 있는 글자는 sentence에 포함되어있는 글자이다.\n",
        "# 또한 모델의 output으로 나올 수 있는 글자도 sentence에 포함되어있는 글자가 되며 이는 sentence의 두번째 글자 부터이다.\n",
        "for sentence in sample_sentences:\n",
        "  x_data = [dic[c] for c in sample_sentences[sample_sentences.index(sentence)][:-1]]\n",
        "  x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
        "  y_data = [dic[c] for c in sample_sentences[sample_sentences.index(sentence)][1:]]\n",
        "\n",
        "# x_one_hot, y_data를 batch(List형)에 넣은 후 Tensor로 변환하는 과정. 모델의 학습은 Tensor를 통해 이루어진다.\n",
        "  input_batch.append(x_one_hot)\n",
        "  target_batch.append(y_data)\n",
        "\n",
        "  IpTs = torch.FloatTensor(input_batch)\n",
        "  tgTs = torch.LongTensor(target_batch)\n",
        "\n",
        "  # Model. 강의 자료에 있는 값이 0.05, 500으로 고정. 더 원활한 학습을 위해서는 파라미터의 수정이 필요하다.\n",
        "  learning_rate = 0.05\n",
        "  training_epochs = 500\n",
        "  model = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "\n",
        "  # define cost/loss & optimizer (분류 과정을 거치므로 CrossEntropyLoss 함수의 도움을 받을 수 있다. Adam은 SGD방식과 유사하지만 더 진화한 알고리즘을 가진다.)\n",
        "  criterion = nn.CrossEntropyLoss()    # Softmax\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # train\n",
        "  for epoch in range(training_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs, _status = model(IpTs)\n",
        "    loss = criterion(outputs.reshape(-1, dic_size), tgTs.reshape(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch %100 == 99:\n",
        "      result = outputs.data.numpy().argmax(axis=2)\n",
        "for sentence in result:\n",
        "        print(''.join([char_set[c] for c in np.squeeze(sentence)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIUFRZJhtvNk",
        "outputId": "e612fc5b-1260-49c9-acb8-9291012341ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "howareyou\n",
            "9\n",
            "whats up?\n",
            "9\n",
            "iamgreat.\n",
            "9\n",
            "['howareyou', 'whats up?', 'iamgreat.']\n",
            "['?', 't', 'm', 'w', 'p', 'e', 'u', 's', 'i', '.', 'y', 'r', 'a', ' ', 'h', 'g', 'o']\n",
            "{'?': 0, 't': 1, 'm': 2, 'w': 3, 'p': 4, 'e': 5, 'u': 6, 's': 7, 'i': 8, '.': 9, 'y': 10, 'r': 11, 'a': 12, ' ': 13, 'h': 14, 'g': 15, 'o': 16}\n",
            "17\n",
            "17\n",
            "owareaou\n",
            "hats up?\n",
            "amgreat.\n"
          ]
        }
      ]
    }
  ]
}