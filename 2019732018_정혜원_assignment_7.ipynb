{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "\n",
        "### Imports"
      ],
      "metadata": {
        "id": "PULXMgBrdjMp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fom9994WNqtx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "eba68ccc-e00a-463a-a9a9-a106343d2779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ff4c3ea05d0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "import torch.optim as opt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Seed 고정\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Data loading\n",
        "\n",
        "|x|y|\n",
        "|:--:|:--:|\n",
        "|1|1|\n",
        "|2|2|\n",
        "|3|3|"
      ],
      "metadata": {
        "id": "WtMo3SCOdnq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data 수동으로 입력하기\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[1], [2], [3]])\n",
        "\n",
        "print(x_train)\n",
        "print(x_train.shape)\n",
        "print(y_train)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JpBmtfXhccCu",
        "outputId": "59fcc23d-273c-4dbe-a3b5-8922b090a353"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.]])\n",
            "torch.Size([3, 1])\n",
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.]])\n",
            "torch.Size([3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Hypothesis (가정) and Cost\n",
        "\n",
        "$$ y_{hypo} = H(x) = w ⋅ x + b $$\n",
        "\n",
        "- Linear(선형)임을 가정한 경우 위 수식을 만족해야 한다.\n",
        "- data가 3개인 경우,\n",
        "  $$\n",
        "\t\\begin{bmatrix} \n",
        "\ty_1 \\\\\n",
        "  y_2 \\\\\n",
        "  y_3 \\\\\n",
        "\t\\end{bmatrix}\n",
        "  = w ⋅\n",
        "  \\begin{bmatrix} \n",
        "  x_1 \\\\\n",
        "  x_2 \\\\\n",
        "  x_3 \\\\\n",
        "\t\\end{bmatrix}\n",
        "  + b\n",
        "\t$$\n",
        "  + 이를 만족하는 $w$와 $b$를 구해야 한다.\n",
        "- Cost function으로 Mean-Square-Error를 이용한다.\n",
        "  $$ \\cfrac{1}{N} \\sum_{n=1}^{N} = (y_{hypo,(n)} - y_{train,(n)})^2 $$"
      ],
      "metadata": {
        "id": "_8DdQMFrd0Ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 초기화 (입력 dim, 출력 dim)\n",
        "model = nn.Linear(1, 1)\n",
        "\n",
        "# Hypothesis 정의\n",
        "y_hypo = model(x_train)\n",
        "print(list(model.parameters()))\n",
        "print(y_hypo)\n",
        "\n",
        "# Cost(Mean Square Error) 계산\n",
        "cost = func.mse_loss(y_hypo, y_train)\n",
        "print(cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FJUaAAl8cm5a",
        "outputId": "99a5df60-7e1a-4f44-c9b1-fb46b8aadd72"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.4414], requires_grad=True)]\n",
            "tensor([[0.0739],\n",
            "        [0.5891],\n",
            "        [1.1044]], grad_fn=<AddmmBackward0>)\n",
            "tensor(2.1471, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Cost Optimization\n",
        "- Optimizer로 stochastic gradient descent를 이용한다"
      ],
      "metadata": {
        "id": "xOV1vONvkQS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer 설정 (learning rate = 0.01로 설정)\n",
        "optimizer = opt.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "CZ8yGnGbcpT8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 반복\n",
        "for epoch in range(1000):\n",
        "\n",
        "  # Cost 계산 / mse_loss(가정에 의한 값, 참값)\n",
        "  y_hypo = model(x_train)\n",
        "  cost = func.mse_loss(y_hypo, y_train)\n",
        "\n",
        "  # Cost를 이용해 model update\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # 100번 마다 중간결과 출력\n",
        "  if epoch % 100 == 99:\n",
        "    params = list(model.parameters())\n",
        "    w = params[0].item()\n",
        "    b = params[1].item()\n",
        "    print('Epoch {:4d}/{} w: {:.3f} b: {:.3f} Cost: {:.6f}'.format(epoch+1, 1000, w, b, cost.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aHZURSHJcrnF",
        "outputId": "bcc9d0dc-4bf1-4e52-8346-600b8155aa9b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  100/1000 w: 1.066 b: -0.150 Cost: 0.003255\n",
            "Epoch  200/1000 w: 1.052 b: -0.118 Cost: 0.002011\n",
            "Epoch  300/1000 w: 1.041 b: -0.093 Cost: 0.001243\n",
            "Epoch  400/1000 w: 1.032 b: -0.073 Cost: 0.000768\n",
            "Epoch  500/1000 w: 1.025 b: -0.057 Cost: 0.000475\n",
            "Epoch  600/1000 w: 1.020 b: -0.045 Cost: 0.000293\n",
            "Epoch  700/1000 w: 1.016 b: -0.035 Cost: 0.000181\n",
            "Epoch  800/1000 w: 1.012 b: -0.028 Cost: 0.000112\n",
            "Epoch  900/1000 w: 1.010 b: -0.022 Cost: 0.000069\n",
            "Epoch 1000/1000 w: 1.008 b: -0.017 Cost: 0.000043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Multivariable Linear Regression\n",
        "\n",
        "$$ y_{hypo} = H(x) = W ⋅ x + b $$\n",
        "\n",
        "- Linear(선형)임을 가정한 경우 위 수식을 만족해야 한다\n",
        "\n",
        "$$ y_{hypo} =\n",
        "  \\begin{bmatrix} \n",
        "\tw_1 && w_2 && w_3 \\\\\n",
        "\t\\end{bmatrix}\n",
        "  ⋅\n",
        "  \\begin{bmatrix} \n",
        "\tx_1 \\\\\n",
        "  x_2 \\\\\n",
        "  x_3 \\\\\n",
        "\t\\end{bmatrix}\n",
        "  +\n",
        "  \\begin{bmatrix} \n",
        "\tb \\\\\n",
        "\t\\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "  + MSE cost를 최소화하는 $w$와 $b$를 구해야 한다"
      ],
      "metadata": {
        "id": "q50Db2zekZH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numpy를 이용해 csv file을 ndarray로 가져오기\n",
        "dataset = np.loadtxt('/content/drive/MyDrive/Colab Notebooks/data_linear_regression.csv', delimiter=',', dtype=np.float32)\n",
        "\n",
        "# 순서를 random으로 섞기\n",
        "np.random.shuffle(dataset)\n",
        "\n",
        "# torch tensor로 변환\n",
        "x_train = torch.FloatTensor(dataset[:,:-1])\n",
        "y_train = torch.FloatTensor(dataset[:,[-1]])\n",
        "\n",
        "# Linear model\n",
        "model = nn.Linear(3, 1)\n",
        "\n",
        "# Optimizer 설정\n",
        "optimizer = opt.SGD(model.parameters(), lr=0.00001)\n",
        "\n",
        "# 반복\n",
        "for epoch in range(1000):\n",
        "  y_hypo = model(x_train)\n",
        "  cost = func.mse_loss(y_hypo, y_train)\n",
        "\n",
        "  # model update\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # 100번 마다 중간결과 출력\n",
        "  if epoch % 100 == 99:\n",
        "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch+1, 1000, cost.item()))\n",
        "\n",
        "print(y_hypo - y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NWy9lysxcxtE",
        "outputId": "51684f34-eb19-45a2-93c5-530b0a4a4bee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  100/1000 Cost: 27.945921\n",
            "Epoch  200/1000 Cost: 26.145187\n",
            "Epoch  300/1000 Cost: 24.495098\n",
            "Epoch  400/1000 Cost: 22.982988\n",
            "Epoch  500/1000 Cost: 21.597193\n",
            "Epoch  600/1000 Cost: 20.327085\n",
            "Epoch  700/1000 Cost: 19.162960\n",
            "Epoch  800/1000 Cost: 18.095854\n",
            "Epoch  900/1000 Cost: 17.117683\n",
            "Epoch 1000/1000 Cost: 16.220892\n",
            "tensor([[-3.7555],\n",
            "        [ 1.7373],\n",
            "        [ 2.5786],\n",
            "        [ 1.0688],\n",
            "        [ 2.0861],\n",
            "        [-2.3550],\n",
            "        [-0.7313],\n",
            "        [-8.5924],\n",
            "        [-1.6001],\n",
            "        [-5.1900],\n",
            "        [-4.2887],\n",
            "        [ 5.1373],\n",
            "        [ 8.3312],\n",
            "        [-2.7923],\n",
            "        [-0.5103],\n",
            "        [ 5.7041],\n",
            "        [ 2.7283],\n",
            "        [-6.4294],\n",
            "        [-4.5285],\n",
            "        [ 3.1929],\n",
            "        [ 3.0094],\n",
            "        [-0.2428],\n",
            "        [ 2.9134],\n",
            "        [-1.0264],\n",
            "        [ 3.7314]], grad_fn=<SubBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Assignment\n",
        "- 행렬 방정식 풀기\n",
        "  + 다음 행렬 방정식을 'Linear Regression'을 이용해 풀어보자\n",
        "    * 적당한 learning rate를 찾아 1000 epoch 정도 계산해본다\n",
        "    * 'Pseudo Inverse'를 이용한 풀이와 비교해본다\n",
        "  + Hint : $y = wx$ 꼴로 변환해본다\n",
        "    * $Ax = b$에서는 x가 미지수이지만, $y = wx$에서는 w가 미지수임에 주의!\n",
        "    * linear model에서 b를 없애기 위해서 nn.Linear() 사용법을 검색해보자"
      ],
      "metadata": {
        "id": "6dZblp96lQg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed 고정 (Linear Regression이 잘 사용이 되었는지 확인하기 위해 (랜덤성을 줄이기 위해)시드를 고정 시킨다.)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Data 입력\n",
        "a_train = torch.FloatTensor([[0,1],[1,1],[2,1],[3,1]])       # Tensor A 정의\n",
        "b_train = torch.FloatTensor([[-1],[0.2],[0.9],[2.1]])        # Tensor B 정의\n",
        "print(a_train)\n",
        "print(b_train)\n",
        "\n",
        "# Model 초기화 (입력 dim, 출력 dim)\n",
        "model = nn.Linear(2, 1, False)    # False -> bias 없애기\n",
        "print(list(model.parameters()))\n",
        "print(model.parameters())\n",
        "\n",
        "# Optimizer 설정 (learning rate = 0.01로 설정) , Stochastic gradient descent 모델을 사용하며 Multivariable function 이므로 Learning rate를 0.013으로 한 개의 입력일 때 보다 낮추어 설정한다.\n",
        "optimizer = opt.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "# 반복\n",
        "for epoch in range(1000):                            \n",
        "\n",
        "  # Cost 계산 / mse_loss (가정에 의한 값, 참값)\n",
        "  b_hypo = model(a_train)                             # y_hypo 정의\n",
        "  cost = func.mse_loss(b_hypo, b_train)               # cost 계산\n",
        "\n",
        "  # cost를 이용해 model update\n",
        "  optimizer.zero_grad()                               # parameter의 gradient 계산 시 0으로 초기화 해준다.\n",
        "  cost.backward()                                     # 목표값과 측정값을 비교한 뒤 오차를 업데이트한다.\n",
        "  optimizer.step()                                    # 계산한 gradient로 parameter를 업데이트한다.\n",
        "\n",
        "  # 100번마다 중간결과 출력\n",
        "  if epoch % 100 == 99:\n",
        "    params = list(model.parameters())\n",
        "    w = params[0][0][0].item()                              \n",
        "    b = params[0][0][1].item()\n",
        "\n",
        "    print('Epoch {:4d}/{} w: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
        "        epoch+1, 1000, w, b, cost.item()))\n",
        "\n",
        "print(b_train)\n",
        "print(b_hypo)\n",
        "\n",
        "print(b_hypo - b_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "f1e227hLc1ZI",
        "outputId": "993ada5b-026c-4b6a-a6e2-646074e41fc3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 1.],\n",
            "        [1., 1.],\n",
            "        [2., 1.],\n",
            "        [3., 1.]])\n",
            "tensor([[-1.0000],\n",
            "        [ 0.2000],\n",
            "        [ 0.9000],\n",
            "        [ 2.1000]])\n",
            "[Parameter containing:\n",
            "tensor([[ 0.3643, -0.3121]], requires_grad=True)]\n",
            "<generator object Module.parameters at 0x7ff4ba045f50>\n",
            "Epoch  100/1000 w: 0.802 b: -0.528 Cost: 0.078024\n",
            "Epoch  200/1000 w: 0.891 b: -0.717 Cost: 0.032368\n",
            "Epoch  300/1000 w: 0.940 b: -0.822 Cost: 0.018524\n",
            "Epoch  400/1000 w: 0.967 b: -0.879 Cost: 0.014327\n",
            "Epoch  500/1000 w: 0.982 b: -0.911 Cost: 0.013054\n",
            "Epoch  600/1000 w: 0.990 b: -0.929 Cost: 0.012668\n",
            "Epoch  700/1000 w: 0.994 b: -0.938 Cost: 0.012551\n",
            "Epoch  800/1000 w: 0.997 b: -0.944 Cost: 0.012515\n",
            "Epoch  900/1000 w: 0.998 b: -0.946 Cost: 0.012505\n",
            "Epoch 1000/1000 w: 0.999 b: -0.948 Cost: 0.012501\n",
            "tensor([[-1.0000],\n",
            "        [ 0.2000],\n",
            "        [ 0.9000],\n",
            "        [ 2.1000]])\n",
            "tensor([[-0.9480],\n",
            "        [ 0.0511],\n",
            "        [ 1.0501],\n",
            "        [ 2.0492]], grad_fn=<MmBackward0>)\n",
            "tensor([[ 0.0520],\n",
            "        [-0.1489],\n",
            "        [ 0.1501],\n",
            "        [-0.0508]], grad_fn=<SubBackward0>)\n",
            "[[0.9990788698196411], [-0.948033332824707]]\n"
          ]
        }
      ]
    }
  ]
}